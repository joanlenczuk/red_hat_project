{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Red hat project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import chi2_contingency, pointbiserialr\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(filename, path = './data'):\n",
    "    return pd.read_csv(os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_activity = import_csv('act_train.csv')\n",
    "people = import_csv('people.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row in the activity file represents a **unique activity** performed by a person on a certain date. Each activity has a unique activity_id.\n",
    "- The activity file contains several different categories of activities. \n",
    "- **Type 1** activities are different from type 2-7 activities because there are more known chars associated with type 1 activities (**nine in total**) than type 2-7 activities (which have only one associated characteristic).\n",
    "- The two files can be joined together using **person_id** as the common key. \n",
    "- All variables are categorical, with the exception of 'char_38' in the people file, which is a continuous numerical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_date(dataframe, date_col):\n",
    "    \"\"\"\n",
    "    A private function which preprocesses datetime information.\n",
    "    Input:\n",
    "    - dataframe\n",
    "    - name of the date column (object)\n",
    "    \n",
    "    Output:\n",
    "    - dataframe with new date columns: month, year, weekend_flg\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    df['date'] = pd.to_datetime(df[date_col])\n",
    "    df['month']=df['date'].dt.month\n",
    "    df['year']=df['date'].dt.year\n",
    "    df['weekend_flg'] = (df['date'].dt.weekday >= 5).astype(int)\n",
    "    df.drop(['date'], inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `People` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_people(original_df):\n",
    "    \"\"\"\n",
    "    The aim of this function is to prepare `people` df by unifying types of data.\n",
    "    The function takes in a dataframe (specifically `people`) and returns a copy of the given dataframe, but with converted data types (all ints).\n",
    "    \"\"\"\n",
    "    df = _process_date(original_df, 'date')\n",
    "    \n",
    "    for col in list(df.select_dtypes(include='object').columns):\n",
    "        if col.startswith(\"char_\") or col.startswith(\"group_\"):\n",
    "            try:\n",
    "                df[col] = (df[col].apply(lambda x: x.split(\" \")[1]).astype(\"float64\")).astype('int64')\n",
    "                print(f\"{col} converted to int\")\n",
    "            except AttributeError:\n",
    "                print(f\"Can't convert {col} to int\")\n",
    "\n",
    "        elif col.startswith(\"people_\"):\n",
    "            try:\n",
    "                df[col] = (df[col].apply(lambda x: x.split(\"_\")[1]).astype(\"float64\")).astype('int64')\n",
    "                print(f'{col} converted to int')\n",
    "            except AttributeError:\n",
    "                print(f\"Can't convert {col} to int\")\n",
    "                \n",
    "    for col in list(df.select_dtypes(include=['bool', 'float64']).columns):\n",
    "        try:\n",
    "            df[col] = df[col].astype(\"int64\")\n",
    "            print(f\"{col} converted to int\")\n",
    "        except AttributeError:\n",
    "            print(f\"Can't convert {col} to int\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id converted to int\n",
      "char_1 converted to int\n",
      "group_1 converted to int\n",
      "char_2 converted to int\n",
      "char_3 converted to int\n",
      "char_4 converted to int\n",
      "char_5 converted to int\n",
      "char_6 converted to int\n",
      "char_7 converted to int\n",
      "char_8 converted to int\n",
      "char_9 converted to int\n",
      "char_10 converted to int\n",
      "char_11 converted to int\n",
      "char_12 converted to int\n",
      "char_13 converted to int\n",
      "char_14 converted to int\n",
      "char_15 converted to int\n",
      "char_16 converted to int\n",
      "char_17 converted to int\n",
      "char_18 converted to int\n",
      "char_19 converted to int\n",
      "char_20 converted to int\n",
      "char_21 converted to int\n",
      "char_22 converted to int\n",
      "char_23 converted to int\n",
      "char_24 converted to int\n",
      "char_25 converted to int\n",
      "char_26 converted to int\n",
      "char_27 converted to int\n",
      "char_28 converted to int\n",
      "char_29 converted to int\n",
      "char_30 converted to int\n",
      "char_31 converted to int\n",
      "char_32 converted to int\n",
      "char_33 converted to int\n",
      "char_34 converted to int\n",
      "char_35 converted to int\n",
      "char_36 converted to int\n",
      "char_37 converted to int\n"
     ]
    }
   ],
   "source": [
    "people_df = clean_people(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. `Activity` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_activity(original_df):\n",
    "    \"\"\"\n",
    "    The aim of this function is to prepare `activity` df by unifying types of data.\n",
    "    The function takes in a dataframe (specifically `activity`) and returns this dataframe, but with converted data types.\n",
    "    \"\"\" \n",
    "\n",
    "    df = _process_date(original_df, 'date')\n",
    "                               \n",
    "    for col in list(df.select_dtypes(include='object').columns):\n",
    "        if col.endswith(\"_id\"):\n",
    "            if col.startswith(\"activity\"):\n",
    "                try:\n",
    "                    df[f\"{col}_prefix\"] = (df[col].apply(lambda x: x.split(\"_\")[0][-1]).astype(\"float64\")).astype(\"int64\")\n",
    "                    print(f\"{col}_prefix created\")\n",
    "                except AttributeError:\n",
    "                    print(f\"Can't create {col}_prefix\")\n",
    "                try:\n",
    "                    df[col] = (df[col].apply(lambda x: x.split(\"_\")[1]).astype(\"float64\")).astype(\"int64\")\n",
    "                    print(f\"{col} converted to int\")\n",
    "                except AttributeError:\n",
    "                    print(f\"Can't convert {col} to int\")              \n",
    "            elif col.startswith(\"people\"):\n",
    "                try:\n",
    "                    df[col] = (df[col].apply(lambda x: x.split(\"_\")[1]).astype(\"float64\")).astype(\"int64\")\n",
    "                    print(f\"{col} converted to int\")\n",
    "                except AttributeError:\n",
    "                    print(f\"Can't convert {col} to int\")\n",
    "        else:\n",
    "            df[col]= df[col].fillna('type -1')\n",
    "            try:\n",
    "                df[col] = (df[col].apply(lambda x: x.split(\" \")[1]).astype(\"float64\")).astype('int64')\n",
    "                print(f\"{col} converted to int\")\n",
    "            except AttributeError:\n",
    "                print(f\"Can't convert {col} to int\")\n",
    "                \n",
    "    for col in list(df.select_dtypes(include=['bool', 'float64']).columns):\n",
    "        try:\n",
    "            df[col] = df[col].astype(\"int64\")\n",
    "            print(f\"{col} converted to int\")\n",
    "        except AttributeError:\n",
    "            print(f\"Can't convert {col} to int\")\n",
    "    df.loc[:,'activity_index'] = df[['activity_id_prefix', 'activity_id']].apply(tuple, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id converted to int\n",
      "activity_id_prefix created\n",
      "activity_id converted to int\n",
      "activity_category converted to int\n",
      "char_1 converted to int\n",
      "char_2 converted to int\n",
      "char_3 converted to int\n",
      "char_4 converted to int\n",
      "char_5 converted to int\n",
      "char_6 converted to int\n",
      "char_7 converted to int\n",
      "char_8 converted to int\n",
      "char_9 converted to int\n",
      "char_10 converted to int\n"
     ]
    }
   ],
   "source": [
    "train_activity_df= clean_activity(train_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Merging data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is being merged at this point to enable a different level of data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pandas 1.0 feature - convert_dtypes(), to handle missing values\n",
    "red_hat = pd.merge(people_df, train_activity_df, how = 'left', on = 'people_id', suffixes = ('_pep', '_act')).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents an acitivity of a specific person (merged by `people_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting records where outcome or activity_id are NaNs, because they are useless in case of modeling\n",
    "red_hat = red_hat[(pd.isna(red_hat['activity_id'])== False) & (pd.isna(red_hat['outcome'])== False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(red_hat, test_size = 0.22, random_state = 42, stratify = red_hat['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.set_index('activity_index')\n",
    "test_set = test_set.set_index('activity_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique combination of activity features is set as index to enable further identification of predicted outcomes for specific activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1713886, 60)\n",
      "(483405, 60)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "del people_df\n",
    "del train_activity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Correlation between features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between categorical variables will be tested using a number of statistical measures, including **chi-square** and **Cramer's V**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['activity_id_prefix', 'activity_id']\n",
    "cat_cols = ['char_10_act' , 'group_1']\n",
    "target = ['outcome']\n",
    "continuous_cols = ['char_38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables to check correlation between them and target\n",
    "cols_corr = [x for x in list(train_set.columns) if x not in (target+continuous_cols+index_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corr_chi2(df, cols_to_check, target, alpha=0.05):\n",
    "    \"\"\" \n",
    "    The aim of the function is to find correlation between two categorical variables, using chi-square.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dataframe with variables and target, \n",
    "    - a list of categorical columns, \n",
    "    - the name of the column with target (to check the correlation between categorical columns and target), \n",
    "    - an alpha parameter (with default=0.5).\n",
    "    \n",
    "    The output of the function is a list of variables which are highly correlated to the target.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in cols_to_check:\n",
    "\n",
    "        #chi-square value , p-value, degrees of freedom, expected frequencies\n",
    "        stat, p, dof, expected = chi2_contingency(pd.crosstab(df[col],df[target]))\n",
    "        print(f\"{col}: significance={alpha}, p={p}\")\n",
    "\n",
    "        if p <= alpha:\n",
    "            print(f'Target and {col} are associated')\n",
    "            cols_to_drop.append(col)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id: significance=0.05, p=0.0\n",
      "Target and people_id are associated\n",
      "char_1_pep: significance=0.05, p=0.0\n",
      "Target and char_1_pep are associated\n",
      "group_1: significance=0.05, p=0.0\n",
      "Target and group_1 are associated\n",
      "char_2_pep: significance=0.05, p=0.0\n",
      "Target and char_2_pep are associated\n",
      "char_3_pep: significance=0.05, p=0.0\n",
      "Target and char_3_pep are associated\n",
      "char_4_pep: significance=0.05, p=0.0\n",
      "Target and char_4_pep are associated\n",
      "char_5_pep: significance=0.05, p=0.0\n",
      "Target and char_5_pep are associated\n",
      "char_6_pep: significance=0.05, p=0.0\n",
      "Target and char_6_pep are associated\n",
      "char_7_pep: significance=0.05, p=0.0\n",
      "Target and char_7_pep are associated\n",
      "char_8_pep: significance=0.05, p=0.0\n",
      "Target and char_8_pep are associated\n",
      "char_9_pep: significance=0.05, p=0.0\n",
      "Target and char_9_pep are associated\n",
      "char_10_pep: significance=0.05, p=0.0\n",
      "Target and char_10_pep are associated\n",
      "char_11: significance=0.05, p=0.0\n",
      "Target and char_11 are associated\n",
      "char_12: significance=0.05, p=0.0\n",
      "Target and char_12 are associated\n",
      "char_13: significance=0.05, p=0.0\n",
      "Target and char_13 are associated\n",
      "char_14: significance=0.05, p=0.0\n",
      "Target and char_14 are associated\n",
      "char_15: significance=0.05, p=0.0\n",
      "Target and char_15 are associated\n",
      "char_16: significance=0.05, p=0.0\n",
      "Target and char_16 are associated\n",
      "char_17: significance=0.05, p=0.0\n",
      "Target and char_17 are associated\n",
      "char_18: significance=0.05, p=0.0\n",
      "Target and char_18 are associated\n",
      "char_19: significance=0.05, p=0.0\n",
      "Target and char_19 are associated\n",
      "char_20: significance=0.05, p=0.0\n",
      "Target and char_20 are associated\n",
      "char_21: significance=0.05, p=0.0\n",
      "Target and char_21 are associated\n",
      "char_22: significance=0.05, p=0.0\n",
      "Target and char_22 are associated\n",
      "char_23: significance=0.05, p=0.0\n",
      "Target and char_23 are associated\n",
      "char_24: significance=0.05, p=0.0\n",
      "Target and char_24 are associated\n",
      "char_25: significance=0.05, p=0.0\n",
      "Target and char_25 are associated\n",
      "char_26: significance=0.05, p=0.0\n",
      "Target and char_26 are associated\n",
      "char_27: significance=0.05, p=0.0\n",
      "Target and char_27 are associated\n",
      "char_28: significance=0.05, p=0.0\n",
      "Target and char_28 are associated\n",
      "char_29: significance=0.05, p=0.0\n",
      "Target and char_29 are associated\n",
      "char_30: significance=0.05, p=0.0\n",
      "Target and char_30 are associated\n",
      "char_31: significance=0.05, p=0.0\n",
      "Target and char_31 are associated\n",
      "char_32: significance=0.05, p=0.0\n",
      "Target and char_32 are associated\n",
      "char_33: significance=0.05, p=0.0\n",
      "Target and char_33 are associated\n",
      "char_34: significance=0.05, p=0.0\n",
      "Target and char_34 are associated\n",
      "char_35: significance=0.05, p=0.0\n",
      "Target and char_35 are associated\n",
      "char_36: significance=0.05, p=0.0\n",
      "Target and char_36 are associated\n",
      "char_37: significance=0.05, p=0.0\n",
      "Target and char_37 are associated\n",
      "month_pep: significance=0.05, p=0.0\n",
      "Target and month_pep are associated\n",
      "year_pep: significance=0.05, p=0.0\n",
      "Target and year_pep are associated\n",
      "weekend_flg_pep: significance=0.05, p=0.0\n",
      "Target and weekend_flg_pep are associated\n",
      "activity_category: significance=0.05, p=0.0\n",
      "Target and activity_category are associated\n",
      "char_1_act: significance=0.05, p=0.0\n",
      "Target and char_1_act are associated\n",
      "char_2_act: significance=0.05, p=0.0\n",
      "Target and char_2_act are associated\n",
      "char_3_act: significance=0.05, p=1.5334587749178806e-241\n",
      "Target and char_3_act are associated\n",
      "char_4_act: significance=0.05, p=5.31276636871444e-310\n",
      "Target and char_4_act are associated\n",
      "char_5_act: significance=0.05, p=0.0\n",
      "Target and char_5_act are associated\n",
      "char_6_act: significance=0.05, p=1.2784896040819015e-235\n",
      "Target and char_6_act are associated\n",
      "char_7_act: significance=0.05, p=7.7896412060875e-181\n",
      "Target and char_7_act are associated\n",
      "char_8_act: significance=0.05, p=0.0\n",
      "Target and char_8_act are associated\n",
      "char_9_act: significance=0.05, p=0.0\n",
      "Target and char_9_act are associated\n",
      "char_10_act: significance=0.05, p=0.0\n",
      "Target and char_10_act are associated\n",
      "month_act: significance=0.05, p=0.0\n",
      "Target and month_act are associated\n",
      "year_act: significance=0.05, p=0.0\n",
      "Target and year_act are associated\n",
      "weekend_flg_act: significance=0.05, p=0.938919833118813\n"
     ]
    }
   ],
   "source": [
    "chi2_cols = find_corr_chi2(train_set, cols_corr, 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/56 are associated to the target based on chi-squared\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(chi2_cols)}/{len(cols_corr)} are associated to the target based on chi-squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-square test turned out to classify **53/54 categorical variables as associated with the target**. But the small p-values might be associated with very large sample sizes, as Chi-square is sensitive to sample size.\n",
    "\n",
    "Therefore, `chi2_cols` won't be dropped from the dataset and a different approach to the correlation between categorical variables will be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Cramer's V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_corrected_stat(cols_to_check, df, target, thresh):\n",
    "    \"\"\" \n",
    "    The aim of the function is to calculate the corrected version of Cramer's V to find the level of association between categorical variables.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dataframe with categorical variables, \n",
    "    - a list of categorical variables to check, \n",
    "    - the name of the column with target,\n",
    "    - a threshold for Cramer's V values from which strong association will be assumed.\n",
    "    \n",
    "    The result of the function is a list of variables which are strongly associated with the target, according to the Cramer's V values.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in cols_to_check:\n",
    "\n",
    "        confusion_matrix = pd.crosstab(df[col],df[target])\n",
    "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum().sum()\n",
    "        phi2 = chi2/n\n",
    "        r,c = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2 - ((c-1)*(r-1))/(n-1))    \n",
    "        rcorr = r - ((r-1)**2)/(n-1)\n",
    "        ccorr = c - ((c-1)**2)/(n-1)\n",
    "        cramers_v = np.sqrt(phi2corr / min( (ccorr-1), (rcorr-1)))\n",
    "        \n",
    "        if cramers_v > thresh:\n",
    "            print(f'Target and {col} are associated: {round(cramers_v,2)}')\n",
    "            cols_to_drop.append(col)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\text{The association between variables and target will by analized according to the following guidelines:} $$\n",
       "$$ V \\in[0,0.3] \\text{ - weak association} $$\n",
       "$$ V \\in(0.3,0.5] \\text{ - medium association} $$\n",
       "$$ V > 0.5 \\text{ - strong association} $$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$ \\text{The association between variables and target will by analized according to the following guidelines:} $$\n",
    "$$ V \\in[0,0.3] \\text{ - weak association} $$\n",
    "$$ V \\in(0.3,0.5] \\text{ - medium association} $$\n",
    "$$ V > 0.5 \\text{ - strong association} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target and people_id are associated: 0.92\n",
      "Target and group_1 are associated: 0.95\n",
      "Target and char_2_pep are associated: 0.68\n"
     ]
    }
   ],
   "source": [
    "cramer_cols = cramers_corrected_stat(cols_corr, train_set, 'outcome', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people_id', 'group_1', 'char_2_pep']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cramer_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns mentioned in `cramer_cols` will be deleted from the dataset as they are stronly associated with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Pearson correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation coefficient can be calculated either for continuous variables or for a categorical variable which has a 0/1-coding for the categories. This correlation is called **point-biserial correlation coefficient**.\n",
    "\n",
    "It will be calculated for the only continuous variable in the dataset - `char_38` and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_biserial_correlation(df, contin_cols, target, thresh):\n",
    "    \"\"\"\n",
    "    The aim of the function is to calculate a point biserial correlation coefficient and the associated p-value.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dataframe with variables for which we want to test correlation\n",
    "    - a list of continuous variable\n",
    "    - a binary variable (target)\n",
    "    - a threshold for correlation coeficient from which strong correlation will be assumed.\n",
    "    \n",
    "    The result of the function is a list of variables with high correlation.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in contin_cols:\n",
    "    \n",
    "        corr = pointbiserialr(df[col],df[target])[0]\n",
    "        p = pointbiserialr(df[col],df[target])[1]\n",
    "    \n",
    "        if (p<=0.5):\n",
    "            if (abs(corr)>thresh):\n",
    "                print(f\"{col} is correlated with the target: {round(corr,2)}\")\n",
    "                cols_to_drop.append(col)\n",
    "            else:\n",
    "                print(f\"{col} with low correlation\")\n",
    "        else:\n",
    "            print(f\"No statistically significant correlation\")\n",
    "    \n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\text{The point biserial correlation coefficient between variables and target will by analized according to the following guidelines:} $$\n",
       "$$ \\mid P\\mid  \\in[0,0.3] \\text{ - weak association} $$\n",
       "$$ \\mid P\\mid \\in(0.3,0.5] \\text{ - medium association} $$\n",
       "$$ \\mid P\\mid > 0.5 \\text{ - strong association} $$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$ \\text{The point biserial correlation coefficient between variables and target will by analized according to the following guidelines:} $$\n",
    "$$ \\mid P\\mid  \\in[0,0.3] \\text{ - weak association} $$\n",
    "$$ \\mid P\\mid \\in(0.3,0.5] \\text{ - medium association} $$\n",
    "$$ \\mid P\\mid > 0.5 \\text{ - strong association} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_38 is correlated with the target: 0.68\n"
     ]
    }
   ],
   "source": [
    "point_biserial_cols = point_biserial_correlation(train_set, continuous_cols, 'outcome', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['char_38']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_biserial_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between features is also worth checking as it can lead to data leakage during modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_corr_features(df, cat_cols, thresh):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to calculate the corrected version of Cramer's V for all the pairs of categorical variables to identify intercorrelated\n",
    "    variables.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dataframe with features (X)\n",
    "    - a list of categorical variables\n",
    "    - a threshold for classifying variables as correlated\n",
    "    \n",
    "    The output of this function is a list of intercorrelated variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_corr = pd.DataFrame(index = cat_cols, columns = cat_cols, data = 0)\n",
    "    #to avoid duplicates of the pairs of variables, only combinations from the triangle will be taken\n",
    "    df_pairs = pd.DataFrame(df_corr.where(np.triu(np.ones(df_corr.shape), k=1).astype(np.bool)).stack()).reset_index().rename(columns={'level_0':'var1','level_1':'var2'}).iloc[:,0:2]    \n",
    "    combinations = []\n",
    "\n",
    "    #appending to the combinations list unique pairs of variables\n",
    "    for x in range(df_pairs.shape[0]):\n",
    "        combinations.append(list(df_pairs.iloc[x,0:2]))\n",
    "        \n",
    "    \n",
    "    corr_features = []\n",
    "\n",
    "    for pair in combinations:\n",
    "        if (pair[0] not in corr_features) & (pair[1] not in corr_features):\n",
    "            #calculating cramer's v for variables that are not yet correlated with previously checked variables\n",
    "            confusion_matrix = pd.crosstab(df[pair[0]],df[pair[1]])\n",
    "            chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "            n = confusion_matrix.sum().sum()\n",
    "            phi2 = chi2/n\n",
    "            r,c = confusion_matrix.shape\n",
    "            phi2corr = max(0, phi2 - ((c-1)*(r-1))/(n-1))    \n",
    "            rcorr = r - ((r-1)**2)/(n-1)\n",
    "            ccorr = c - ((c-1)**2)/(n-1)\n",
    "            cramers_v = np.sqrt(phi2corr / min( (ccorr-1), (rcorr-1)))\n",
    "\n",
    "            if cramers_v > thresh:\n",
    "                print(f'{pair[0]} and {pair[1]} are associated: {round(cramers_v,2)}, dropping {pair[0]}')\n",
    "                # Only one of the correlated variables will be added to the list of correlated variables.\n",
    "                # The variable is picked randomly as additional calculations on such a big volume of data is to costly.\n",
    "                corr_features.append(pair[0])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id and char_1_pep are associated: 0.96, dropping people_id\n",
      "char_1_pep and char_2_pep are associated: 1.0, dropping char_1_pep\n",
      "group_1 and char_2_pep are associated: 0.88, dropping group_1\n",
      "char_3_pep and char_5_pep are associated: 0.86, dropping char_3_pep\n",
      "char_6_pep and char_7_pep are associated: 0.86, dropping char_6_pep\n",
      "char_8_pep and char_9_pep are associated: 0.86, dropping char_8_pep\n",
      "char_13 and char_19 are associated: 0.82, dropping char_13\n",
      "char_15 and char_34 are associated: 0.82, dropping char_15\n",
      "char_16 and char_20 are associated: 0.84, dropping char_16\n",
      "char_19 and char_21 are associated: 0.82, dropping char_19\n",
      "char_21 and char_22 are associated: 0.82, dropping char_21\n",
      "char_22 and char_23 are associated: 0.83, dropping char_22\n",
      "char_23 and char_37 are associated: 0.81, dropping char_23\n",
      "char_28 and char_37 are associated: 0.81, dropping char_28\n",
      "char_36 and char_37 are associated: 0.86, dropping char_36\n",
      "activity_category and char_10_act are associated: 1.0, dropping activity_category\n",
      "month_act and year_act are associated: 0.91, dropping month_act\n"
     ]
    }
   ],
   "source": [
    "correlated_features = cramers_corr_features(train_set, cols_corr, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Frequency check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all of the modelling features are categorical, frequency check will be performed in order to make sure that there are no variables with high frequency of just one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_check(df, cat_cols, thresh):\n",
    "    \"\"\"\n",
    "    The aim of the function is to find features with categories of high frequency.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dataframe with features\n",
    "    - list of categorical columns\n",
    "    - threshold for high frequency\n",
    "    \n",
    "    The output of the function is a list of variables with categories of high frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    high_freq = []\n",
    "    df_cat = df[cat_cols]\n",
    "    for col in list(df_cat.columns):\n",
    "        max_freq = df_cat[col].value_counts(normalize=True).sort_values(ascending=False).max()\n",
    "        if max_freq > thresh:\n",
    "            high_freq.append(col)\n",
    "            \n",
    "    return high_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_frequency_cols = frequency_check(train_set, cols_corr, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['char_1_pep',\n",
       " 'char_18',\n",
       " 'char_24',\n",
       " 'char_26',\n",
       " 'char_29',\n",
       " 'char_1_act',\n",
       " 'char_2_act',\n",
       " 'char_3_act',\n",
       " 'char_4_act',\n",
       " 'char_5_act',\n",
       " 'char_6_act',\n",
       " 'char_7_act',\n",
       " 'char_8_act',\n",
       " 'char_9_act']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_frequency_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Preparing the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be split to avoid overfitting the pipeline. Additionally, data types must be converted into strings to enable using the encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('outcome', axis=1)\n",
    "y_train = train_set['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_set.drop('outcome', axis=1)\n",
    "y_test = test_set['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_set\n",
    "del test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data types to objects\n",
    "# X_train = X_train.astype(str)\n",
    "# X_test = X_test.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = list(set(index_cols+cat_cols+cramer_cols+point_biserial_cols+correlated_features+high_frequency_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_cols = [x for x in X_train.columns if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. Selecting columns for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration indicated that two categorical features: `char_10_act` and `group_1` have too many categories to encode and therefore they will be dropped. Additionally, features connected to the index itself will be dropped as well, as they bring no value to the modelling process.\n",
    "\n",
    "At this point variables associated with the target will be dropped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts only a given list of columns and returns a filtered dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_names):\n",
    "        self._feature_names = feature_names \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        return X[self._feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3. Filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values will be filled with a constant, in this case ***-1***, as it was previously done in the cleaning data step. The custom ValueImputer class is being created because SimpleImputer outputs a numpy array, while we need a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fills missing values with a constant.\n",
    "    \"\"\"\n",
    "    def __init__(self, impute_value):\n",
    "        self.impute_value = impute_value\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y = None):   \n",
    "        return X.fillna(self.impute_value)\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        return X.fillna(self.impute_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4. Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point all features are categorical. However, they all have a different number of categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of categories for each variable\n",
    "for cat in modelling_cols:\n",
    "    if cat != 'outcome':\n",
    "        categories[cat]= len((list(X_train[cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_10_pep': 2,\n",
       " 'char_11': 2,\n",
       " 'char_12': 2,\n",
       " 'char_14': 2,\n",
       " 'char_17': 2,\n",
       " 'char_20': 2,\n",
       " 'char_25': 2,\n",
       " 'char_27': 2,\n",
       " 'char_30': 2,\n",
       " 'char_31': 2,\n",
       " 'char_32': 2,\n",
       " 'char_33': 2,\n",
       " 'char_34': 2,\n",
       " 'char_35': 2,\n",
       " 'char_37': 2,\n",
       " 'weekend_flg_pep': 2,\n",
       " 'year_act': 2,\n",
       " 'weekend_flg_act': 2,\n",
       " 'year_pep': 4,\n",
       " 'char_5_pep': 9,\n",
       " 'char_9_pep': 9,\n",
       " 'month_pep': 12,\n",
       " 'char_4_pep': 25,\n",
       " 'char_7_pep': 25}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(categories.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of variables are binary, but the number of categories for other features vary.\n",
    "\n",
    "- Variables with **binary categories**: convert to 0-1 values\n",
    "- Variables with **3-10 categories**: frequency encoding\n",
    "- Variables with **10 categories and more**: mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cols_to_encode(cat_dict, thresh):\n",
    "    \"\"\"\n",
    "    Defining the lists of columns to encode depending on the number of categories per feature.\n",
    "    \n",
    "    The function takes in:\n",
    "    - a dictionary with name of columns as keys and number of categories as values\n",
    "    - (min. number -1) of categories to include in frequency encoding and at the same time max. number of categories to include in binary encoding \n",
    "    \"\"\"                       \n",
    "    binary_cat = list({k for k, v in cat_dict.items() if v == 2})\n",
    "    little_cat = list({k for k, v in cat_dict.items() if v in range(3,10)})\n",
    "    big_cat = list({k for k, v in cat_dict.items() if v >= 10})\n",
    "    \n",
    "                          \n",
    "    return little_cat, big_cat, binary_cat                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_cat, big_cat, binary_cat = define_cols_to_encode(categories, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features with binary categories: 18\n",
      "Number of features with less than 10 number of categories: 3\n",
      "Number of features with 10 and more number of categories: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features with binary categories: {len(binary_cat)}\")\n",
    "print(f\"Number of features with less than {thresh} number of categories: {len(little_cat)}\")\n",
    "print(f\"Number of features with {thresh} and more number of categories: {len(big_cat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary categories encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the binary features have values in [0,1], but there are some variables with a different set of categories. Therefore, they all will be encoded into [0,1] values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder01(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables using their frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, binary_cols):\n",
    "        \"\"\"\n",
    "        Freq_cols is a list of columns which will be encoded using the Encoder01\n",
    "        \"\"\"\n",
    "        self.binary_cols = binary_cols\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"\n",
    "        The fit method takes in a DataFrame with features (X) and a numpy array with the target variable (y).\n",
    "        \n",
    "        It creates a dictionary, where keys are names of features and values are dictionaries (zipped uniques&zero_ones).\n",
    "        In the zipped dictionary keys are the names of categories represented by a specific feature and the values are the new binary values : 0 or 1.\n",
    "        \"\"\"\n",
    "        self.maps ={}\n",
    "        for col in self.binary_cols:\n",
    "            self.maps[col] = []\n",
    "            uniques = sorted(list(X[col].unique()))\n",
    "            zero_ones = [0,1]\n",
    "            self.maps[col]  = dict(zip(uniques, zero_ones)) \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        The transform method takes in a DataFrame with features (X) and a numpy array with the target variable (y).\n",
    "        \n",
    "        The transform method replaces the names of categories with zeros or ones (using values stored in `map` dictionary).\n",
    "        If a given category is not in the dictionary, it is encoded with \"-1\".\n",
    "        \"\"\"\n",
    "        \n",
    "        for var in self.maps.keys():\n",
    "            try:\n",
    "                X[var] = X[var].apply(lambda x: self.maps[var][x])\n",
    "            except KeyError:\n",
    "                print(\"Missing key for test set\")\n",
    "                X[var] = X[var].apply(lambda x: -1)\n",
    "        return X\n",
    "        \n",
    "        \n",
    "    def fit_transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Combines the above mentioned fit and transform methods.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables using their frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_cols):\n",
    "        \"\"\"\n",
    "        Freq_cols is a list of columns which will be encoded using the FrequencyEncoder\n",
    "        \"\"\"\n",
    "        self.freq_cols = freq_cols\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"\n",
    "        The fit method takes in a DataFrame with features (X) and a numpy array with the target variable (y).\n",
    "        \n",
    "        It creates a dictionary, where keys are names of features and values are dictionaries (zipped uniques&frequencies).\n",
    "        In the zipped dictionary keys are the names of categories represented by a specific feature and the values are their frequencies of occurance in the set.\n",
    "        \"\"\"\n",
    "        self.maps ={}\n",
    "        for col in self.freq_cols:\n",
    "            self.maps[col] = {}\n",
    "            uniques = list(X[col].unique())\n",
    "            frequencies = list(X.groupby(col).size()/ len(X))\n",
    "            self.maps[col]  = dict(zip(uniques, [round(x,3) for x in frequencies])) \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        The transform method takes in a DataFrame with features (X) and a numpy array with the target variable (y).\n",
    "        \n",
    "        The transform method replaces the names of categories with the frequencies of those categories in the dataset (using values stored in `map` dictionary).\n",
    "        If a given category is not in the dictionary, it is encoded with \"-1\".\n",
    "        \"\"\"\n",
    "        for var in self.maps.keys():\n",
    "            try:\n",
    "                X[var] = X[var].apply(lambda x: self.maps[var][x])\n",
    "            except KeyError:\n",
    "                print(\"Missing key for test set\")\n",
    "                X[var] = X[var].apply(lambda x: -1)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Combines the above mentioned fit and transform methods.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean encoder will be applied without changes from category_encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Transfroming the dataset using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing operations from the pipeline will be fit and transformed using the training data and transformed - using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "        ('column_selector', ColumnSelector(modelling_cols)),\n",
    "        ('imputer', ValueImputer(\"-1\")),\n",
    "        ('binary_encoder', Encoder01(binary_cat)),\n",
    "        ('frequency_encoder', FrequencyEncoder(little_cat)),\n",
    "        ('target_encoder', TargetEncoder(cols = big_cat, smoothing = 0.8))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[little_cat + big_cat] = X_train[little_cat + big_cat].astype('str')\n",
    "X_test[little_cat + big_cat] = X_test[little_cat + big_cat].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joannal/Documents/python_projects/python37_env/lib/python3.7/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "X_train_t = cat_pipeline.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_4_pep</th>\n",
       "      <th>char_5_pep</th>\n",
       "      <th>char_7_pep</th>\n",
       "      <th>char_9_pep</th>\n",
       "      <th>char_10_pep</th>\n",
       "      <th>char_11</th>\n",
       "      <th>char_12</th>\n",
       "      <th>char_14</th>\n",
       "      <th>char_17</th>\n",
       "      <th>char_20</th>\n",
       "      <th>char_25</th>\n",
       "      <th>char_27</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_37</th>\n",
       "      <th>month_pep</th>\n",
       "      <th>year_pep</th>\n",
       "      <th>weekend_flg_pep</th>\n",
       "      <th>year_act</th>\n",
       "      <th>weekend_flg_act</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(2, 3769922)</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 2255070)</th>\n",
       "      <td>0.469</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 1102570)</th>\n",
       "      <td>0.535</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.249</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 4143394)</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 3554173)</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                char_4_pep  char_5_pep  char_7_pep  char_9_pep  char_10_pep  \\\n",
       "activity_index                                                                \n",
       "(2, 3769922)         0.360       0.025       0.467       0.051            0   \n",
       "(2, 2255070)         0.469       0.162       0.519       0.051            0   \n",
       "(2, 1102570)         0.535       0.064       0.390       0.249            1   \n",
       "(2, 4143394)         0.360       0.025       0.467       0.051            0   \n",
       "(2, 3554173)         0.360       0.025       0.390       0.051            0   \n",
       "\n",
       "                char_11  char_12  char_14  char_17  char_20  char_25  char_27  \\\n",
       "activity_index                                                                  \n",
       "(2, 3769922)          0        0        0        0        0        0        0   \n",
       "(2, 2255070)          0        0        0        0        0        0        0   \n",
       "(2, 1102570)          0        1        1        1        0        1        1   \n",
       "(2, 4143394)          0        0        0        0        0        0        0   \n",
       "(2, 3554173)          0        0        0        0        0        0        0   \n",
       "\n",
       "                char_30  char_31  char_32  char_33  char_34  char_35  char_37  \\\n",
       "activity_index                                                                  \n",
       "(2, 3769922)          0        0        0        0        0        0        0   \n",
       "(2, 2255070)          0        0        0        0        0        0        0   \n",
       "(2, 1102570)          1        1        1        1        1        1        0   \n",
       "(2, 4143394)          0        0        0        0        0        0        0   \n",
       "(2, 3554173)          0        0        0        0        0        0        0   \n",
       "\n",
       "                month_pep  year_pep  weekend_flg_pep  year_act  \\\n",
       "activity_index                                                   \n",
       "(2, 3769922)        0.576     0.091                0         0   \n",
       "(2, 2255070)        0.477     0.091                0         0   \n",
       "(2, 1102570)        0.450     0.091                0         0   \n",
       "(2, 4143394)        0.500     0.189                1         1   \n",
       "(2, 3554173)        0.450     0.091                0         0   \n",
       "\n",
       "                weekend_flg_act  \n",
       "activity_index                   \n",
       "(2, 3769922)                  0  \n",
       "(2, 2255070)                  1  \n",
       "(2, 1102570)                  0  \n",
       "(2, 4143394)                  1  \n",
       "(2, 3554173)                  0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = cat_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_4_pep</th>\n",
       "      <th>char_5_pep</th>\n",
       "      <th>char_7_pep</th>\n",
       "      <th>char_9_pep</th>\n",
       "      <th>char_10_pep</th>\n",
       "      <th>char_11</th>\n",
       "      <th>char_12</th>\n",
       "      <th>char_14</th>\n",
       "      <th>char_17</th>\n",
       "      <th>char_20</th>\n",
       "      <th>char_25</th>\n",
       "      <th>char_27</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_37</th>\n",
       "      <th>month_pep</th>\n",
       "      <th>year_pep</th>\n",
       "      <th>weekend_flg_pep</th>\n",
       "      <th>year_act</th>\n",
       "      <th>weekend_flg_act</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(2, 2791174)</th>\n",
       "      <td>0.386</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 4219859)</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.249</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.166</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 2522827)</th>\n",
       "      <td>0.386</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 908393)</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 4370791)</th>\n",
       "      <td>0.557</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                char_4_pep  char_5_pep  char_7_pep  char_9_pep  char_10_pep  \\\n",
       "activity_index                                                                \n",
       "(2, 2791174)         0.386       0.069       0.898       0.249            0   \n",
       "(2, 4219859)         0.360       0.025       0.519       0.249            1   \n",
       "(2, 2522827)         0.386       0.115       0.467       0.051            0   \n",
       "(2, 908393)          0.360       0.025       0.104       0.051            0   \n",
       "(2, 4370791)         0.557       0.055       0.059       0.051            0   \n",
       "\n",
       "                char_11  char_12  char_14  char_17  char_20  char_25  char_27  \\\n",
       "activity_index                                                                  \n",
       "(2, 2791174)          1        0        0        1        1        1        0   \n",
       "(2, 4219859)          1        0        0        1        1        1        0   \n",
       "(2, 2522827)          0        0        0        0        0        0        0   \n",
       "(2, 908393)           0        0        0        0        0        0        0   \n",
       "(2, 4370791)          0        0        0        0        0        0        0   \n",
       "\n",
       "                char_30  char_31  char_32  char_33  char_34  char_35  char_37  \\\n",
       "activity_index                                                                  \n",
       "(2, 2791174)          0        0        1        0        1        0        1   \n",
       "(2, 4219859)          0        0        1        0        1        0        1   \n",
       "(2, 2522827)          0        0        0        0        0        0        0   \n",
       "(2, 908393)           0        0        0        0        0        0        0   \n",
       "(2, 4370791)          0        0        0        0        0        0        0   \n",
       "\n",
       "                month_pep  year_pep  weekend_flg_pep  year_act  \\\n",
       "activity_index                                                   \n",
       "(2, 2791174)        0.346     0.091                1         0   \n",
       "(2, 4219859)        0.500     0.166                1         1   \n",
       "(2, 2522827)        0.576     0.189                0         1   \n",
       "(2, 908393)         0.500     0.554                0         1   \n",
       "(2, 4370791)        0.346     0.091                0         0   \n",
       "\n",
       "                weekend_flg_act  \n",
       "activity_index                   \n",
       "(2, 2791174)                  0  \n",
       "(2, 4219859)                  0  \n",
       "(2, 2522827)                  0  \n",
       "(2, 908393)                   0  \n",
       "(2, 4370791)                  0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving datasets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.to_csv('./csv_files/red_hat_train.csv')\n",
    "X_test_t.to_csv('./csv_files/red_hat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).to_csv('./csv_files/outcome_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('./csv_files/outcome_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37_env",
   "language": "python",
   "name": "python37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
